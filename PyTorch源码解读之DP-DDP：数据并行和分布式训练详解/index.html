<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
<meta name="google-site-verification" content="1aOZ-Mqx4wpQNb6_FZv6-oqB1p4VGmQ0nlu-EuBSzgU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/header2.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/header2.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/header2.jpg">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zsh4614.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="引言：本文介绍 PyTorch 里的数据并行训练，涉及 nn.DataParallel (DP) 和 nn.parallel.DistributedDataParallel (DDP) 两个模块（基于 1.7 版本），涵盖分布式训练的原理以及源码解读。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch源码解读之DP&amp;DDP：数据并行和分布式训练详解">
<meta property="og:url" content="https://zsh4614.cn/PyTorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BDP-DDP%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="欢迎来到我的主页!">
<meta property="og:description" content="引言：本文介绍 PyTorch 里的数据并行训练，涉及 nn.DataParallel (DP) 和 nn.parallel.DistributedDataParallel (DDP) 两个模块（基于 1.7 版本），涵盖分布式训练的原理以及源码解读。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=n">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m_j">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=m_j+%3D+%5Cfrac%7Bn%7D%7Bk%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=l">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5C+Loss%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial%5B%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dl%28x_i%2Cy_i%29%5D%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cfrac%7B%5Cpartial+l%28x_i%2Cy_i%29%7D%7B%5Cpartial+w%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7Bm_j%7D%7Bn%7D+%5Cfrac%7B%5Cpartial%5B%5Cfrac%7B1%7D%7Bm_j%7D%5Csum_%7Bi%3D+m_%7Bj-1%7D%7D%5E%7Bm_%7Bj-1%7D%2Bm_%7Bj%7D%7Dl%28x_i%2Cy_i%29%5D%7D%7B%5Cpartial+w%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7Bm_j%7D%7Bn%7D%5Cfrac%7B%5Cpartial%5C+loss_%7Bj%7D%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B1%7D%7Bk%7D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7B%5Cpartial%5C+loss_%7Bj%7D%7D%7B%5Cpartial+w%7D">
<meta property="og:image" content="https://i.loli.net/2021/05/15/E4lF2jLGf1N8SPV.jpg">
<meta property="og:image" content="https://i.loli.net/2021/05/15/9GVSsC2prZ3PiOq.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=k">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7Bp%7D%7Bb%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T+%3D+2%28k-1%29%5Cfrac%7Bp%7D%7Bb%7D+">
<meta property="og:image" content="https://i.loli.net/2021/05/16/kOYXy9deM3oUG4c.png">
<meta property="og:image" content="https://i.loli.net/2021/05/16/ctNmUGaEBXT2uA5.gif">
<meta property="og:image" content="https://i.loli.net/2021/05/16/clDb6TRnw1BPHf2.gif">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=2%28k-1%29%5Cfrac%7B%5Cfrac%7Bp%7D%7Bk%7D%7D%7Bb%7D">
<meta property="og:image" content="https://i.loli.net/2021/05/17/umiJGts7Y2qfjzI.png">
<meta property="og:image" content="https://i.loli.net/2021/05/17/D7NZUTiWJ23qfBb.png">
<meta property="article:published_time" content="2021-05-15T14:18:57.000Z">
<meta property="article:modified_time" content="2021-12-01T17:30:30.013Z">
<meta property="article:author" content="zsh">
<meta property="article:tag" content="DP">
<meta property="article:tag" content="DDP">
<meta property="article:tag" content="源码解读">
<meta property="article:tag" content="分布式训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=n">

<link rel="canonical" href="https://zsh4614.cn/PyTorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BDP-DDP%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E8%AF%A6%E8%A7%A3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch源码解读之DP&DDP：数据并行和分布式训练详解 | 欢迎来到我的主页!</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="欢迎来到我的主页!" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">欢迎来到我的主页!</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-resource">

    <a href="/categories/%E5%A4%96%E9%83%A8%E8%B5%84%E6%BA%90/" rel="section"><i class="fa fa-server fa-fw"></i>资源</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zsh4614.cn/PyTorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%E4%B9%8BDP-DDP%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header2.jpg">
      <meta itemprop="name" content="zsh">
      <meta itemprop="description" content="怕什么真理无穷<br>进一寸有一寸的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="欢迎来到我的主页!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch源码解读之DP&DDP：数据并行和分布式训练详解
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-15 22:18:57" itemprop="dateCreated datePublished" datetime="2021-05-15T22:18:57+08:00">2021-05-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-02 01:30:30" itemprop="dateModified" datetime="2021-12-02T01:30:30+08:00">2021-12-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>39k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>35 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>引言：本文介绍 PyTorch 里的数据并行训练，涉及 nn.DataParallel (DP) 和 nn.parallel.DistributedDataParallel (DDP) 两个模块（基于 1.7 版本），涵盖分布式训练的原理以及源码解读。</p>
<span id="more"></span>
<h3 id="数据并行">数据并行</h3>
<p>当一张 GPU 可以存储一个模型时，可以采用数据并行得到更准确的梯度或者加速训练，即每个 GPU 复制一份模型，将一批样本分为多份输入各个模型并行计算。因为<strong>求导以及加和都是线性的</strong>，数据并行在数学上也有效。</p>
<p>假设我们一个 batch 有 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]"> 个样本，一共有 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个 GPU 每个 GPU 分到 <img src="https://www.zhihu.com/equation?tex=m_j" alt="[公式]"> 个样本。假设样本刚好等分，则有 <img src="https://www.zhihu.com/equation?tex=m_j+%3D+%5Cfrac%7Bn%7D%7Bk%7D" alt="[公式]"> 。我们考虑总的损失函数 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]"> 对参数 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 的导数：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5C+Loss%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial%5B%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dl%28x_i%2Cy_i%29%5D%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cfrac%7B%5Cpartial+l%28x_i%2Cy_i%29%7D%7B%5Cpartial+w%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7Bm_j%7D%7Bn%7D+%5Cfrac%7B%5Cpartial%5B%5Cfrac%7B1%7D%7Bm_j%7D%5Csum_%7Bi%3D+m_%7Bj-1%7D%7D%5E%7Bm_%7Bj-1%7D%2Bm_%7Bj%7D%7Dl%28x_i%2Cy_i%29%5D%7D%7B%5Cpartial+w%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7Bm_j%7D%7Bn%7D%5Cfrac%7B%5Cpartial%5C+loss_%7Bj%7D%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B1%7D%7Bk%7D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D+%5Cfrac%7B%5Cpartial%5C+loss_%7Bj%7D%7D%7B%5Cpartial+w%7D" alt="[公式]"></p>
<p>那么接下来我们看一下 PyTorch 究竟是怎么实现数据并行的。</p>
<h3 id="DP">DP</h3>
<h4 id="使用">使用</h4>
<p>DP 的好处是，使用起来非常方便，只需要将原来单卡的 model 用 DP 改成多卡:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure>
<h4 id="原理">原理</h4>
<p>DP 基于单机多卡，所有设备都负责计算和训练网络，除此之外， device[0] (并非 GPU 真实标号而是输入参数 device_ids 首位) 还要负责整合梯度，更新参数。图 1 即为 GPU 0 作为 device[0] 的例子。从图中我们可以看出，有三个主要过程：</p>
<ul>
<li>过程一（图中红色部分）：各卡分别计算损失和梯度</li>
<li>过程二（图中蓝色部分）：所有梯度整合到 device[0]</li>
<li>过程三（图中绿色部分）：device[0] 进行参数更新，其他卡复制 device[0] 的参数</li>
</ul>
<p><img src="https://i.loli.net/2021/05/15/E4lF2jLGf1N8SPV.jpg" alt="ps.jpg"></p>
<p>DP 只能实现单机训练，使用的是多线程而非多个进程，不能算是严格意义上的分布式训练（多个节点），但是其原理和分布式训练算法里的 Parameter Server 架构很相近，我们借用 PS 的伪代码来说明一下。</p>
<p><img src="https://i.loli.net/2021/05/15/9GVSsC2prZ3PiOq.jpg" alt="4089rq9421004o77o6nr15nq9s5sq2p0.jpg"></p>
<p>我们可以看到 PS 的并行梯度下降流程分为：</p>
<ul>
<li>
<p>Task Scheduler：负责加载数据并分发数据至每个 worker 节点，并执行多轮迭代</p>
</li>
<li>
<p>Worker：①初始化：载入数据并将全部模型参数从 server 节点拉下来；②梯度计算：利用该节点的数据计算梯度并将梯度更新到 server 节点</p>
</li>
<li>
<p>Server：①汇总梯度；②更新参数</p>
</li>
</ul>
<p>OK， 现在我们已经知道了 DP 使用的算法，接下来我们看一下 PyTorch 是如何实现的。</p>
<h4 id="实现">实现</h4>
<p>这一节主要讨论 DP 的实现，首先先贴上源码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataParallel</span>(<span class="params">Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, module, device_ids=<span class="literal">None</span>, output_device=<span class="literal">None</span>, dim=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DataParallel, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查是否有可用的 GPU</span></span><br><span class="line">        device_type = _get_available_device_type()</span><br><span class="line">        <span class="keyword">if</span> device_type <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.module = module</span><br><span class="line">            self.device_ids = []</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># 默认使用所有可见的 GPU</span></span><br><span class="line">        <span class="keyword">if</span> device_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            device_ids = _get_all_device_indices()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 默认 server 是 device_ids 列表上第一个</span></span><br><span class="line">        <span class="keyword">if</span> output_device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            output_device = device_ids[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.module = module</span><br><span class="line">        self.device_ids = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: _get_device_index(x, <span class="literal">True</span>), device_ids))</span><br><span class="line">        self.output_device = _get_device_index(output_device, <span class="literal">True</span>)</span><br><span class="line">        self.src_device_obj = torch.device(device_type, self.device_ids[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查负载是否平衡， 不平衡（指内存或者处理器 max/min &gt; 0.75 会有警告）</span></span><br><span class="line">        _check_balance(self.device_ids)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 单卡</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.device_ids) == <span class="number">1</span>:</span><br><span class="line">            self.module.to(self.src_device_obj)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *inputs, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 没 GPU 可用</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.device_ids:</span><br><span class="line">            <span class="keyword">return</span> self.module(*inputs, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 运行前 GPU device_ids[0] （即我们的 server）上必须有 parallelized module 的parameters 和 buffers</span></span><br><span class="line">        <span class="comment"># 因为 DP 保证 GPU device_ids[0] 和 base parallelized module 共享存储</span></span><br><span class="line">        <span class="comment"># 所以在device[0] 上的 in-place 更新也会被保留下来，其他的则不会</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> chain(self.module.parameters(), self.module.buffers()):</span><br><span class="line">            <span class="keyword">if</span> t.device != self.src_device_obj:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;module must have its parameters and buffers &quot;</span></span><br><span class="line">                                   <span class="string">&quot;on device &#123;&#125; (device_ids[0]) but found one of &quot;</span></span><br><span class="line">                                   <span class="string">&quot;them on device: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.src_device_obj, t.device))</span><br><span class="line"></span><br><span class="line">         <span class="comment"># nice 现在 device[0] 上已经有了 module 和 input， 接下来我们就要开始 PS 算法了</span></span><br><span class="line">        <span class="comment"># 可以开始看正文了</span></span><br><span class="line"></span><br><span class="line">        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果仅有单卡可用，直接单卡计算，不用并行</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.device_ids) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> self.module(*inputs[<span class="number">0</span>], **kwargs[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        replicas = self.replicate(self.module, self.device_ids[:<span class="built_in">len</span>(inputs)])</span><br><span class="line">        outputs = self.parallel_apply(replicas, inputs, kwargs)</span><br><span class="line">        <span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replicate</span>(<span class="params">self, module, device_ids</span>):</span></span><br><span class="line">        <span class="keyword">return</span> replicate(module, device_ids, <span class="keyword">not</span> torch.is_grad_enabled())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter</span>(<span class="params">self, inputs, kwargs, device_ids</span>):</span></span><br><span class="line">        <span class="keyword">return</span> scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parallel_apply</span>(<span class="params">self, replicas, inputs, kwargs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> parallel_apply(replicas, inputs, kwargs, self.device_ids[:<span class="built_in">len</span>(replicas)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gather</span>(<span class="params">self, outputs, output_device</span>):</span></span><br><span class="line">        <span class="keyword">return</span> gather(outputs, output_device, dim=self.dim)</span><br></pre></td></tr></table></figure>
<p>从 forward 函数可以看出，关键函数有 scatter, replicate, parallel_apply 和 gather，我们一个一个看一下。</p>
<p>首先是 scatter 函数，即 scatter_kwargs 函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter_kwargs</span>(<span class="params">inputs, kwargs, target_gpus, dim=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Scatter with support for kwargs dictionary&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主要函数</span></span><br><span class="line">    inputs = scatter(inputs, target_gpus, dim) <span class="keyword">if</span> inputs <span class="keyword">else</span> []</span><br><span class="line">    kwargs = scatter(kwargs, target_gpus, dim) <span class="keyword">if</span> kwargs <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用空项补全使 inputs 和 kwargs 长度相当</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(inputs) &lt; <span class="built_in">len</span>(kwargs):</span><br><span class="line">        inputs.extend([() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(kwargs) - <span class="built_in">len</span>(inputs))])</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(kwargs) &lt; <span class="built_in">len</span>(inputs):</span><br><span class="line">        kwargs.extend([&#123;&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(inputs) - <span class="built_in">len</span>(kwargs))])</span><br><span class="line">    <span class="comment"># 返回 tuple</span></span><br><span class="line">    inputs = <span class="built_in">tuple</span>(inputs)</span><br><span class="line">    kwargs = <span class="built_in">tuple</span>(kwargs)</span><br><span class="line">    <span class="keyword">return</span> inputs, kwargs</span><br></pre></td></tr></table></figure>
<p>scatter_kwargs 函数中最重要的就是 scatter 函数，负责将 tensor 分成大概相等的块并将他们分给不同的 GPU。对其他的数据类型，则是复制分散给不同的 GPU 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scatter</span>(<span class="params">inputs, target_gpus, dim=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Slices tensors into approximately equal chunks and</span></span><br><span class="line"><span class="string">    distributes them across given GPUs. Duplicates</span></span><br><span class="line"><span class="string">    references to objects that are not tensors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scatter_map</span>(<span class="params">obj</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> Scatter.apply(target_gpus, <span class="literal">None</span>, dim, obj)</span><br><span class="line">        <span class="keyword">if</span> is_namedtuple(obj):</span><br><span class="line">            <span class="keyword">return</span> [<span class="built_in">type</span>(obj)(*args) <span class="keyword">for</span> args <span class="keyword">in</span> <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj))]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj)))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="built_in">list</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj))]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(obj, <span class="built_in">dict</span>) <span class="keyword">and</span> <span class="built_in">len</span>(obj) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="built_in">type</span>(obj)(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">zip</span>(*<span class="built_in">map</span>(scatter_map, obj.items()))]</span><br><span class="line">        <span class="keyword">return</span> [obj <span class="keyword">for</span> targets <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># After scatter_map is called, a scatter_map cell will exist. This cell</span></span><br><span class="line">    <span class="comment"># has a reference to the actual function scatter_map, which has references</span></span><br><span class="line">    <span class="comment"># to a closure that has a reference to the scatter_map cell (because the</span></span><br><span class="line">    <span class="comment"># fn is recursive). To avoid this reference cycle, we set the function to</span></span><br><span class="line">    <span class="comment"># None, clearing the cell</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = scatter_map(inputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        scatter_map = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>其中，针对 tensor 的函数，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scatter</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, target_gpus, chunk_sizes, dim, <span class="built_in">input</span></span>):</span></span><br><span class="line">        target_gpus = [_get_device_index(x, <span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> target_gpus]</span><br><span class="line">        ctx.dim = dim</span><br><span class="line">        ctx.input_device = <span class="built_in">input</span>.get_device() <span class="keyword">if</span> <span class="built_in">input</span>.device.<span class="built_in">type</span> != <span class="string">&quot;cpu&quot;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        streams = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">and</span> ctx.input_device == -<span class="number">1</span>:</span><br><span class="line">            <span class="comment"># Perform CPU to GPU copies in a background stream</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 新建 cuda stream</span></span><br><span class="line">            streams = [_get_stream(device) <span class="keyword">for</span> device <span class="keyword">in</span> target_gpus]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 真正的操作</span></span><br><span class="line">        outputs = comm.scatter(<span class="built_in">input</span>, target_gpus, chunk_sizes, ctx.dim, streams)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Synchronize with the copy stream</span></span><br><span class="line">        <span class="keyword">if</span> streams <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">for</span> i, output <span class="keyword">in</span> <span class="built_in">enumerate</span>(outputs):</span><br><span class="line">                <span class="keyword">with</span> torch.cuda.device(target_gpus[i]):</span><br><span class="line">                    main_stream = torch.cuda.current_stream()</span><br><span class="line">                    main_stream.wait_stream(streams[i])</span><br><span class="line">                    output.record_stream(main_stream)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, *grad_output</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, Gather.apply(ctx.input_device, ctx.dim, *grad_output)</span><br></pre></td></tr></table></figure>
<p>comm.scatter 依赖于 C++，就不介绍了。</p>
<p>回顾 DP 代码块，我们已经运行完 scatter函数，即将一个 batch 近似等分成更小的 batch。接下来我们要看 replicate 函数和 gather 函数 （假设我们有不少于两张卡）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DP forward 里的代码</span></span><br><span class="line">   replicas = self.replicate(self.module, self.device_ids[:<span class="built_in">len</span>(inputs)])</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 实现</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">replicate</span>(<span class="params">network, devices, detach=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> _replicatable_module(network):</span><br><span class="line">           <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Cannot replicate network where python modules are &quot;</span></span><br><span class="line">                              <span class="string">&quot;childrens of ScriptModule&quot;</span>)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> <span class="keyword">not</span> devices:</span><br><span class="line">           <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 需要复制到哪些 GPU， 复制多少份</span></span><br><span class="line">       devices = [_get_device_index(x, <span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> devices]</span><br><span class="line">       num_replicas = <span class="built_in">len</span>(devices)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 复制 parameters</span></span><br><span class="line">       params = <span class="built_in">list</span>(network.parameters())</span><br><span class="line">       param_indices = &#123;param: idx <span class="keyword">for</span> idx, param <span class="keyword">in</span> <span class="built_in">enumerate</span>(params)&#125;</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 拉到代码块底部看原函数，然后再回来</span></span><br><span class="line">       param_copies = _broadcast_coalesced_reshape(params, devices, detach)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment"># 复制 buffers</span></span><br><span class="line">       buffers = <span class="built_in">list</span>(network.buffers())</span><br><span class="line">       buffers_rg = []</span><br><span class="line">       buffers_not_rg = []</span><br><span class="line">       <span class="keyword">for</span> buf <span class="keyword">in</span> buffers:</span><br><span class="line">           <span class="keyword">if</span> buf.requires_grad <span class="keyword">and</span> <span class="keyword">not</span> detach:</span><br><span class="line">               buffers_rg.append(buf)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               buffers_not_rg.append(buf)</span><br><span class="line"></span><br><span class="line">               <span class="comment"># 记录需要和不需要求导的 buffer 的 index</span></span><br><span class="line">       buffer_indices_rg = &#123;buf: idx <span class="keyword">for</span> idx, buf <span class="keyword">in</span> <span class="built_in">enumerate</span>(buffers_rg)&#125;</span><br><span class="line">       buffer_indices_not_rg = &#123;buf: idx <span class="keyword">for</span> idx, buf <span class="keyword">in</span> <span class="built_in">enumerate</span>(buffers_not_rg)&#125;</span><br><span class="line"></span><br><span class="line">               <span class="comment"># 分别拷贝，这个咱们已经会了</span></span><br><span class="line">       buffer_copies_rg = _broadcast_coalesced_reshape(buffers_rg, devices, detach=detach)</span><br><span class="line">       buffer_copies_not_rg = _broadcast_coalesced_reshape(buffers_not_rg, devices, detach=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 现在开始拷贝网络</span></span><br><span class="line">       <span class="comment"># 准备过程：将 network.modules() 变成list</span></span><br><span class="line">       <span class="comment"># 然后再为之后复制的模型准备好空的 list 和 indices</span></span><br><span class="line"></span><br><span class="line">       modules = <span class="built_in">list</span>(network.modules())</span><br><span class="line">       module_copies = [[] <span class="keyword">for</span> device <span class="keyword">in</span> devices]</span><br><span class="line">       module_indices = &#123;&#125;</span><br><span class="line">       scriptmodule_skip_attr = &#123;<span class="string">&quot;_parameters&quot;</span>, <span class="string">&quot;_buffers&quot;</span>, <span class="string">&quot;_modules&quot;</span>, <span class="string">&quot;forward&quot;</span>, <span class="string">&quot;_c&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(modules):</span><br><span class="line">           module_indices[module] = i</span><br><span class="line">           <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">               replica = module._replicate_for_data_parallel()</span><br><span class="line">               <span class="comment"># This is a temporary fix for DDP. DDP needs to access the</span></span><br><span class="line">               <span class="comment"># replicated model parameters. It used to do so through</span></span><br><span class="line">               <span class="comment"># `mode.parameters()`. The fix added in #33907 for DP stops the</span></span><br><span class="line">               <span class="comment"># `parameters()` API from exposing the replicated parameters.</span></span><br><span class="line">               <span class="comment"># Hence, we add a `_former_parameters` dict here to support DDP.</span></span><br><span class="line">               replica._former_parameters = OrderedDict()</span><br><span class="line"></span><br><span class="line">               module_copies[j].append(replica)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 接下来分别复制 module，param，buffer</span></span><br><span class="line">       <span class="keyword">for</span> i, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(modules):</span><br><span class="line">           <span class="keyword">for</span> key, child <span class="keyword">in</span> module._modules.items():</span><br><span class="line">               <span class="keyword">if</span> child <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       replica._modules[key] = <span class="literal">None</span></span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   module_idx = module_indices[child]</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       <span class="built_in">setattr</span>(replica, key, module_copies[j][module_idx])</span><br><span class="line">           <span class="keyword">for</span> key, param <span class="keyword">in</span> module._parameters.items():</span><br><span class="line">               <span class="keyword">if</span> param <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       replica._parameters[key] = <span class="literal">None</span></span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   param_idx = param_indices[param]</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       param = param_copies[j][param_idx]</span><br><span class="line">                       <span class="comment"># parameters in replicas are no longer leaves,</span></span><br><span class="line">                       <span class="comment"># so setattr them as non-parameter attributes</span></span><br><span class="line">                       <span class="built_in">setattr</span>(replica, key, param)</span><br><span class="line">                       <span class="comment"># expose the parameter for DDP</span></span><br><span class="line">                       replica._former_parameters[key] = param</span><br><span class="line">           <span class="keyword">for</span> key, buf <span class="keyword">in</span> module._buffers.items():</span><br><span class="line">               <span class="keyword">if</span> buf <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       replica._buffers[key] = <span class="literal">None</span></span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   <span class="keyword">if</span> buf.requires_grad <span class="keyword">and</span> <span class="keyword">not</span> detach:</span><br><span class="line">                       buffer_copies = buffer_copies_rg</span><br><span class="line">                       buffer_idx = buffer_indices_rg[buf]</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       buffer_copies = buffer_copies_not_rg</span><br><span class="line">                       buffer_idx = buffer_indices_not_rg[buf]</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">                       replica = module_copies[j][i]</span><br><span class="line">                       <span class="built_in">setattr</span>(replica, key, buffer_copies[j][buffer_idx])</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> [module_copies[j][<span class="number">0</span>] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas)]</span><br><span class="line"></span><br><span class="line">   <span class="comment"># ！！！从replicate来看这里</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">_broadcast_coalesced_reshape</span>(<span class="params">tensors, devices, detach=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">from</span> ._functions <span class="keyword">import</span> Broadcast</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 先看 else 的 comment，因为不 detach 也会用到同样的函数</span></span><br><span class="line">     <span class="keyword">if</span> detach:</span><br><span class="line">         <span class="keyword">return</span> comm.broadcast_coalesced(tensors, devices)</span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         <span class="comment"># Use the autograd function to broadcast if not detach</span></span><br><span class="line">         <span class="keyword">if</span> <span class="built_in">len</span>(tensors) &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">           <span class="comment"># 下拉看源码</span></span><br><span class="line">             tensor_copies = Broadcast.apply(devices, *tensors)</span><br><span class="line"></span><br><span class="line">             <span class="keyword">return</span> [tensor_copies[i:i + <span class="built_in">len</span>(tensors)]</span><br><span class="line">                     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(tensor_copies), <span class="built_in">len</span>(tensors))]</span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">             <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">  <span class="comment">#  Broadcast.apply</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Broadcast</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">   @staticmethod</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, target_gpus, *inputs</span>):</span></span><br><span class="line">       <span class="keyword">assert</span> <span class="built_in">all</span>(i.device.<span class="built_in">type</span> != <span class="string">&#x27;cpu&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> inputs), (</span><br><span class="line">           <span class="string">&#x27;Broadcast function not implemented for CPU tensors&#x27;</span></span><br><span class="line">       )</span><br><span class="line">       target_gpus = [_get_device_index(x, <span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> target_gpus]</span><br><span class="line">       ctx.target_gpus = target_gpus</span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">len</span>(inputs) == <span class="number">0</span>:</span><br><span class="line">           <span class="keyword">return</span> <span class="built_in">tuple</span>()</span><br><span class="line">       ctx.num_inputs = <span class="built_in">len</span>(inputs)</span><br><span class="line">       <span class="comment"># input 放在 device[0]</span></span><br><span class="line">       ctx.input_device = inputs[<span class="number">0</span>].get_device()</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 和 detach 的情况一样</span></span><br><span class="line">       outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># comm.broadcast_coalesced 的代码</span></span><br><span class="line">       <span class="comment"># tensors 必须在同一个设备，CPU 或者 GPU； devices 即是要拷贝到的设备；buffer_size 则是最大的buffer</span></span><br><span class="line">       <span class="comment"># 这里用到 buffer 将小张量合并到缓冲区以减少同步次数</span></span><br><span class="line">       <span class="comment"># def broadcast_coalesced(tensors, devices, buffer_size=10485760):</span></span><br><span class="line">       <span class="comment">#    devices = [_get_device_index(d) for d in devices]</span></span><br><span class="line">           <span class="comment">#       return torch._C._broadcast_coalesced(tensors, devices, buffer_size)</span></span><br><span class="line"></span><br><span class="line">       non_differentiables = []</span><br><span class="line">       <span class="keyword">for</span> idx, input_requires_grad <span class="keyword">in</span> <span class="built_in">enumerate</span>(ctx.needs_input_grad[<span class="number">1</span>:]):</span><br><span class="line">           <span class="keyword">if</span> <span class="keyword">not</span> input_requires_grad:</span><br><span class="line">               <span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">                   non_differentiables.append(output[idx])</span><br><span class="line">       ctx.mark_non_differentiable(*non_differentiables)</span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">tuple</span>([t <span class="keyword">for</span> tensors <span class="keyword">in</span> outputs <span class="keyword">for</span> t <span class="keyword">in</span> tensors])</span><br><span class="line"></span><br><span class="line"><span class="meta">   @staticmethod</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, *grad_outputs</span>):</span></span><br><span class="line">       <span class="keyword">return</span> (<span class="literal">None</span>,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)</span><br></pre></td></tr></table></figure>
<p>下面继续 parallel_apply 部分。⚠️ DP 和 DDP 共用 parallel_apply 代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DP 代码</span></span><br><span class="line">outputs = self.parallel_apply(replicas, inputs, kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># threading 实现，用前面准备好的 replica 和输入数据，然后</span></span><br><span class="line"><span class="comment"># for 循环启动多线程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 源码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallel_apply</span>(<span class="params">modules, inputs, kwargs_tup=<span class="literal">None</span>, devices=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个 GPU 都有模型和输入</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(modules) == <span class="built_in">len</span>(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确保每个 GPU 都有相应的数据，如没有就空白补全</span></span><br><span class="line">    <span class="keyword">if</span> kwargs_tup <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      	<span class="comment"># 咱们在 scatter 已经补全了</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(modules) == <span class="built_in">len</span>(kwargs_tup)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        kwargs_tup = (&#123;&#125;,) * <span class="built_in">len</span>(modules)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> devices <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(modules) == <span class="built_in">len</span>(devices)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        devices = [<span class="literal">None</span>] * <span class="built_in">len</span>(modules)</span><br><span class="line"></span><br><span class="line">    devices = [_get_device_index(x, <span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> devices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 多线程实现</span></span><br><span class="line"></span><br><span class="line">    lock = threading.Lock()</span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    grad_enabled, autocast_enabled = torch.is_grad_enabled(), torch.is_autocast_enabled()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义 worker</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_worker</span>(<span class="params">i, module, <span class="built_in">input</span>, kwargs, device=<span class="literal">None</span></span>):</span></span><br><span class="line">        torch.set_grad_enabled(grad_enabled)</span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            device = get_a_var(<span class="built_in">input</span>).get_device()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.cuda.device(device), autocast(enabled=autocast_enabled):</span><br><span class="line">                <span class="comment"># this also avoids accidental slicing of `input` if it is a Tensor</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(<span class="built_in">input</span>, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">                    <span class="built_in">input</span> = (<span class="built_in">input</span>,)</span><br><span class="line">                output = module(*<span class="built_in">input</span>, **kwargs)</span><br><span class="line">            <span class="keyword">with</span> lock:</span><br><span class="line">                <span class="comment"># 并行计算得到输出</span></span><br><span class="line">                results[i] = output</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">with</span> lock:</span><br><span class="line">                results[i] = ExceptionWrapper(</span><br><span class="line">                    where=<span class="string">&quot;in replica &#123;&#125; on device &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, device))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(modules) &gt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 如有一个进程控制多个 GPU ，起多个线程</span></span><br><span class="line">      <span class="comment"># 需要强调一下，虽然 DDP 推荐单卡单进程，即每次调用 DDP device_ids 都只输入一张卡的 id（通常是 args.local_rank），但是如果输入多个 device_id，此时 DDP 就是单进程多线程控制多卡，和 DP 一样，关于 DDP 的解读可以看下文</span></span><br><span class="line"></span><br><span class="line">        threads = [threading.Thread(target=_worker,</span><br><span class="line">                                    args=(i, module, <span class="built_in">input</span>, kwargs, device))</span><br><span class="line">                   <span class="keyword">for</span> i, (module, <span class="built_in">input</span>, kwargs, device) <span class="keyword">in</span></span><br><span class="line">                   <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(modules, inputs, kwargs_tup, devices))]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">            thread.start()</span><br><span class="line">        <span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">            thread.join()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># 一个 GPU 一个进程 （ DDP 推荐操作）</span></span><br><span class="line">        _worker(<span class="number">0</span>, modules[<span class="number">0</span>], inputs[<span class="number">0</span>], kwargs_tup[<span class="number">0</span>], devices[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(inputs)):</span><br><span class="line">        output = results[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># error handle</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(output, ExceptionWrapper):</span><br><span class="line">            output.reraise()</span><br><span class="line">        outputs.append(output)</span><br><span class="line">    <span class="comment"># 输出 n 个计算结果</span></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>现在我们已经得到并行计算的结果了，接下来我们要将结果收集到 device[0]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DP 代码</span></span><br><span class="line"><span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br><span class="line"><span class="comment"># 收集到 devices[0]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 源码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather</span>(<span class="params">outputs, target_device, dim=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Gathers tensors from different GPUs on a specified device</span></span><br><span class="line"><span class="string">      (-1 means the CPU).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gather_map</span>(<span class="params">outputs</span>):</span></span><br><span class="line">        out = outputs[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(out, torch.Tensor):</span><br><span class="line">            <span class="keyword">return</span> Gather.apply(target_device, dim, *outputs)</span><br><span class="line">        <span class="keyword">if</span> out <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(out, <span class="built_in">dict</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>((<span class="built_in">len</span>(out) == <span class="built_in">len</span>(d) <span class="keyword">for</span> d <span class="keyword">in</span> outputs)):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&#x27;All dicts must have the same number of keys&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">type</span>(out)(((k, gather_map([d[k] <span class="keyword">for</span> d <span class="keyword">in</span> outputs]))</span><br><span class="line">                              <span class="keyword">for</span> k <span class="keyword">in</span> out))</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">type</span>(out)(<span class="built_in">map</span>(gather_map, <span class="built_in">zip</span>(*outputs)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recursive function calls like this create reference cycles.</span></span><br><span class="line">    <span class="comment"># Setting the function to None clears the refcycle.</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = gather_map(outputs)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        gather_map = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gather 源码</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gather</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, target_device, dim, *inputs</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">all</span>(i.device.<span class="built_in">type</span> != <span class="string">&#x27;cpu&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> inputs), (</span><br><span class="line">            <span class="string">&#x27;Gather function not implemented for CPU tensors&#x27;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        target_device = _get_device_index(target_device, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        ctx.target_device = target_device</span><br><span class="line"></span><br><span class="line">        ctx.dim = dim</span><br><span class="line">        ctx.input_gpus = <span class="built_in">tuple</span>(i.get_device() <span class="keyword">for</span> i <span class="keyword">in</span> inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>(t.dim() == <span class="number">0</span> <span class="keyword">for</span> t <span class="keyword">in</span> inputs) <span class="keyword">and</span> dim == <span class="number">0</span>:</span><br><span class="line">            inputs = <span class="built_in">tuple</span>(t.view(<span class="number">1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> inputs)</span><br><span class="line">            warnings.warn(<span class="string">&#x27;Was asked to gather along dimension 0, but all &#x27;</span></span><br><span class="line">                          <span class="string">&#x27;input tensors were scalars; will instead unsqueeze &#x27;</span></span><br><span class="line">                          <span class="string">&#x27;and return a vector.&#x27;</span>)</span><br><span class="line">            ctx.unsqueezed_scalar = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ctx.unsqueezed_scalar = <span class="literal">False</span></span><br><span class="line">        ctx.input_sizes = <span class="built_in">tuple</span>(i.size(ctx.dim) <span class="keyword">for</span> i <span class="keyword">in</span> inputs)</span><br><span class="line">        <span class="keyword">return</span> comm.gather(inputs, ctx.dim, ctx.target_device)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, grad_output</span>):</span></span><br><span class="line">        scattered_grads = Scatter.apply(ctx.input_gpus, ctx.input_sizes, ctx.dim, grad_output)</span><br><span class="line">        <span class="keyword">if</span> ctx.unsqueezed_scalar:</span><br><span class="line">            scattered_grads = <span class="built_in">tuple</span>(g[<span class="number">0</span>] <span class="keyword">for</span> g <span class="keyword">in</span> scattered_grads)</span><br><span class="line">        <span class="keyword">return</span> (<span class="literal">None</span>, <span class="literal">None</span>) + scattered_grads</span><br><span class="line"></span><br><span class="line"><span class="comment"># comm.gather 涉及到 C++，具体实现咱也不讲了 ；)  </span></span><br><span class="line"><span class="comment"># Gathers tensors from multiple GPU devices.   </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather</span>(<span class="params">tensors, dim=<span class="number">0</span>, destination=<span class="literal">None</span>, *, out=<span class="literal">None</span></span>):</span></span><br><span class="line">    tensors = [_handle_complex(t) <span class="keyword">for</span> t <span class="keyword">in</span> tensors]</span><br><span class="line">    <span class="keyword">if</span> out <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> destination == -<span class="number">1</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&#x27;Using -1 to represent CPU tensor is deprecated. Please use a &#x27;</span></span><br><span class="line">                <span class="string">&#x27;device object or string instead, e.g., &quot;cpu&quot;.&#x27;</span>)</span><br><span class="line">        destination = _get_device_index(destination, allow_cpu=<span class="literal">True</span>, optional=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> torch._C._gather(tensors, dim, destination)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> destination <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">                <span class="string">&quot;&#x27;destination&#x27; must not be specified when &#x27;out&#x27; is specified, but &quot;</span></span><br><span class="line">                <span class="string">&quot;got destination=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(destination))</span><br><span class="line">        <span class="keyword">return</span> torch._C._gather_out(tensors, out, dim)</span><br></pre></td></tr></table></figure>
<p>前向传播的时候我们会先用 Scatter 函数将数据从 device[0] 分配并复制到不同的卡，之后用 Replicate 函数将模型从 device[0] 复制到不同的卡，之后各个卡都有了同样的模型和不同的数据，分别调用 forward 计算损失和梯度。</p>
<p>反向传播的时候，我们会将梯度收集到 device[0] 然后在 device[0] 更新参数。</p>
<h4 id="分析">分析</h4>
<ul>
<li>
<p>负载不均衡</p>
<p>device[0] 负载大一些</p>
</li>
<li>
<p>通信开销</p>
<p>假设有 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 个 GPU， 完成一次通信需要时间 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bp%7D%7Bb%7D" alt="[公式]"> ，那么使用 PS 算法，总共需要花费时间 <img src="https://www.zhihu.com/equation?tex=T+%3D+2%28k-1%29%5Cfrac%7Bp%7D%7Bb%7D+" alt="[公式]"></p>
</li>
<li>
<p>单进程</p>
<p>The difference between <code>DistributedDataParallel</code> and <code>DataParallel</code> is: <code>DistributedDataParallel</code> uses multiprocessing where a process is created for each GPU, while <code>DataParallel</code> uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.</p>
<p>全局解释器锁，简单来说就是，一个 Python 进程只能利用一个 CPU kernel，即单核多线程并发时，只能执行一个线程。考虑多核，多核多线程可能出现线程颠簸 (thrashing) 造成资源浪费，所以 Python 想要利用多核最好是多进程。</p>
</li>
</ul>
<h3 id="DDP">DDP</h3>
<h4 id="使用-2">使用</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--save_dir&quot;</span>, default=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=-<span class="number">1</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--world_size&quot;</span>, default=<span class="number">1</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化后端</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># world_size 指的是总的并行进程数目</span></span><br><span class="line"><span class="comment"># 比如16张卡单卡单进程 就是 16</span></span><br><span class="line"><span class="comment"># 等到连接的进程数等于world_size，程序才会继续运行</span></span><br><span class="line">torch.distributed.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>,</span><br><span class="line">                                         world_size=ws,</span><br><span class="line">                                         init_method=<span class="string">&#x27;env://&#x27;</span>)</span><br><span class="line"></span><br><span class="line">torch.cuda.set_device(args.local_rank)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;args.local_rank&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model = nn.Linear(<span class="number">2</span>,<span class="number">3</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train dataset</span></span><br><span class="line"><span class="comment"># train_sampler</span></span><br><span class="line"><span class="comment"># train_loader</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 DDP，这里我们通过规定 device_id 用了单卡单进程</span></span><br><span class="line"><span class="comment"># 实际上根据我们前面对 parallel_apply 的解读，DDP 也支持一个进程控制多个线程利用多卡</span></span><br><span class="line">model = DDP(model,</span><br><span class="line">            device_ids=[args.local_rank],</span><br><span class="line">            output_device=args.local_rank).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型 </span></span><br><span class="line"><span class="keyword">if</span> torch.distributed.get_rank() == <span class="number">0</span>:</span><br><span class="line">  torch.save(model.module.state_dict(),</span><br><span class="line">             <span class="string">&#x27;results/%s/model.pth&#x27;</span> % args.save_dir)</span><br></pre></td></tr></table></figure>
<h4 id="原理-2">原理</h4>
<p>和DP的区别：</p>
<p>1.多进程：和 DP 不同， DDP 采用多进程，最推荐的做法是每张卡一个进程从而避免上一节所说单进程带来的影响。前文也提到了 DP 和 DDP 共用一个 parallel_apply 函数，所以 DDP 同样支持单进程多线程多卡操作，自然也支持多进程多线程，不过需要注意一下 world_size。</p>
<p>2.通信效率：DP 的通信成本随着 GPU 数量线性增长，而 DDP 支持 Ring AllReduce，其通信成本是恒定的，与 GPU 数量无关。</p>
<p>3.同步参数：DP 通过收集梯度到 device[0]，在device[0] 更新参数，然后其他设备复制 device[0] 的参数实现各个模型同步；DDP 通过保证初始状态相同并且改变量也相同（指同步梯度） ，保证模型同步。</p>
<p>Ring AllReduce模型如下所示：</p>
<p><img src="https://i.loli.net/2021/05/16/kOYXy9deM3oUG4c.png" alt="ring-gpus.png"></p>
<p>Ring AllReduce是一种利用带宽优化环解决通信问题的方法，解决了DP通信成本和GPU的数量线性相关的问题，分为两个步骤：Scatter Reduce和All Gather</p>
<p>Scatter Reduce过程：首先，我们将参数分为k份，相邻的GPU传递不同的参数，在传递k-1次之后，可以得到每一份参数的累积（在不同的GPU上）。</p>
<p><img src="https://i.loli.net/2021/05/16/ctNmUGaEBXT2uA5.gif" alt="v2-4590aeb5fd981b1e6f926cc68605884a_b.gif"></p>
<p>All Gather：得到每一份参数的累积之后，再做一次传递，同步到所有的GPU上。</p>
<p><img src="https://i.loli.net/2021/05/16/clDb6TRnw1BPHf2.gif" alt="v2-c9df34575d7d95ec87d85575d25d6f37_b.gif"></p>
<p>根据这两个过程，我们可以计算到All Reduce的通信成本为： <img src="https://www.zhihu.com/equation?tex=2%28k-1%29%5Cfrac%7B%5Cfrac%7Bp%7D%7Bk%7D%7D%7Bb%7D" alt="[公式]"> ，和 GPU 数量无关了。</p>
<p>DDP 也是数据并行，所以每张卡都有模型和输入。我们以多进程多线程为例，每起一个进程，该进程的 device[0] 都会从本地复制模型，如果该进程仍有多线程，就像 DP，模型会从 device[0] 复制到其他设备。</p>
<p>DDP 通过 Reducer 来管理梯度同步。为了提高通讯效率， Reducer 会将梯度归到不同的桶里（按照模型参数的 reverse order， 因为反向传播需要符合这样的顺序），一次归约一个桶。其中桶的大小为参数 bucket_cap_mb 默认为 25，可根据需要调整。下图即为一个例子。</p>
<p>可以看到每个进程里，模型参数都按照倒序放在桶里，每次归约一个桶。</p>
<p><img src="https://i.loli.net/2021/05/17/umiJGts7Y2qfjzI.png" alt="72401724-d296d880-371a-11ea-90ab-737f86543df9.png"></p>
<p>终于可以看 DDP 的实现了！！首先我们贴上伪代码！</p>
<p><img src="https://i.loli.net/2021/05/17/D7NZUTiWJ23qfBb.png" alt="Screenshot from 2021-05-17 00-23-12.png"></p>
<p>从 DDP 的伪代码我们可以看出，DDP 最重要的包括三部分：</p>
<ul>
<li>
<p>constructor：负责在构建的时候将 rank 0 的 state_dict() 广播 ➜ 保证所有网络初始状态相同；初始化 buckets 并尽可能按逆序将 parameters 分配进 buckets ➜ 按桶通信提高效率；为每个 parameter 加上 grad_accumulator 以及在 autograd_graph 注册 autograd_hook ➜ 在 backward 时负责梯度同步。</p>
</li>
<li>
<p>forward：正常的 forward 操作；如果 self.find_unused_parameters 设置为 True，DDP 会在 forward 结束时 traverse autograd graph 找到所有没用过的parameters 并标记为 ready ➜ 虽说这一步开销很大，但是有时计算动态图会改变，所以很必要。</p>
</li>
<li>
<p>autograd_hook：这个 hook 是挂在 autograd graph 在 backward 时负责梯度同步的。当一个梯度计算好后，相应的 hook 会告诉 DDP 可以用来归约。当一个桶里的梯度都可以了，Reducer 就会启动异步 allreduce 去计算所有进程的平均值。当所有桶都可以了，Reducer 会等所有 allreduce 完成，然后将得到的梯度写到 param.grad。</p>
</li>
</ul>
<p>好的，但现在为止我们应该对 DDP 有了大致了解了，接下来就一起看一下代码是怎么实现的！</p>
<ul>
<li>通信：因为 DDP 依赖 c10d 的 ProcessGroup 进行通信，所以开始前我们先要有个 ProcessGroup 实例。这步可以通过 torch.distributed.init_process_group 实现。</li>
<li>构建：我们先贴上 DDP 初始化的源码，最重要的是 _ddp_init_helper 这个函数，负责多线程时复制模型、将 parameters 分组、创建 reducer 以及为 SyncBN 做准备等。这部分代码看 comment 就能懂，我们会重点说一下 dist.Reducer，作为管理器，自然很重要了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DistributedDataParallel</span>(<span class="params">Module</span>):</span>       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, module, device_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 output_device=<span class="literal">None</span>, dim=<span class="number">0</span>, broadcast_buffers=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 process_group=<span class="literal">None</span>,  </span></span></span><br><span class="line"><span class="function"><span class="params">                 bucket_cap_mb=<span class="number">25</span>,       </span></span></span><br><span class="line"><span class="function"><span class="params">                 find_unused_parameters=<span class="literal">False</span>,       </span></span></span><br><span class="line"><span class="function"><span class="params">                 check_reduction=<span class="literal">False</span>,      </span></span></span><br><span class="line"><span class="function"><span class="params">                 gradient_as_bucket_view=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(DistributedDataParallel, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">any</span>((p.requires_grad <span class="keyword">for</span> p <span class="keyword">in</span> module.parameters())), (</span><br><span class="line">            <span class="string">&quot;DistributedDataParallel is not needed when a module &quot;</span></span><br><span class="line">            <span class="string">&quot;doesn&#x27;t have any parameter that requires a gradient.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.is_multi_device_module = <span class="built_in">len</span>(&#123;p.device <span class="keyword">for</span> p <span class="keyword">in</span> module.parameters()&#125;) &gt; <span class="number">1</span></span><br><span class="line">        distinct_device_types = &#123;p.device.<span class="built_in">type</span> <span class="keyword">for</span> p <span class="keyword">in</span> module.parameters()&#125;</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(distinct_device_types) == <span class="number">1</span>, (</span><br><span class="line">            <span class="string">&quot;DistributedDataParallel&#x27;s input module must be on &quot;</span></span><br><span class="line">            <span class="string">&quot;the same type of devices, but input module parameters locate in &#123;&#125;.&quot;</span></span><br><span class="line">        ).<span class="built_in">format</span>(distinct_device_types)</span><br><span class="line">        self.device_type = <span class="built_in">list</span>(distinct_device_types)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.device_type == <span class="string">&quot;cpu&quot;</span> <span class="keyword">or</span> self.is_multi_device_module:</span><br><span class="line">            <span class="keyword">assert</span> <span class="keyword">not</span> device_ids <span class="keyword">and</span> <span class="keyword">not</span> output_device, (</span><br><span class="line">                <span class="string">&quot;DistributedDataParallel device_ids and output_device arguments &quot;</span></span><br><span class="line">                <span class="string">&quot;only work with single-device GPU modules, but got &quot;</span></span><br><span class="line">                <span class="string">&quot;device_ids &#123;&#125;, output_device &#123;&#125;, and module parameters &#123;&#125;.&quot;</span></span><br><span class="line">            ).<span class="built_in">format</span>(device_ids, output_device, &#123;p.device <span class="keyword">for</span> p <span class="keyword">in</span> module.parameters()&#125;)</span><br><span class="line"></span><br><span class="line">            self.device_ids = <span class="literal">None</span></span><br><span class="line">            self.output_device = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Use all devices by default for single-device GPU modules</span></span><br><span class="line">            <span class="keyword">if</span> device_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                device_ids = _get_all_device_indices()</span><br><span class="line"></span><br><span class="line">            self.device_ids = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: _get_device_index(x, <span class="literal">True</span>), device_ids))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> output_device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                output_device = device_ids[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            self.output_device = _get_device_index(output_device, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> process_group <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.process_group = _get_default_group()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.process_group = process_group</span><br><span class="line"></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.module = module</span><br><span class="line">        self.device = <span class="built_in">list</span>(self.module.parameters())[<span class="number">0</span>].device</span><br><span class="line">        self.broadcast_buffers = broadcast_buffers</span><br><span class="line">        self.find_unused_parameters = find_unused_parameters</span><br><span class="line">        self.require_backward_grad_sync = <span class="literal">True</span></span><br><span class="line">        self.require_forward_param_sync = <span class="literal">True</span></span><br><span class="line">        self.ddp_join_enabled = <span class="literal">False</span></span><br><span class="line">        self.gradient_as_bucket_view = gradient_as_bucket_view</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> check_reduction:</span><br><span class="line">            <span class="comment"># This argument is no longer used since the reducer</span></span><br><span class="line">            <span class="comment"># will ensure reduction completes even if some parameters</span></span><br><span class="line">            <span class="comment"># do not receive gradients.</span></span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;The `check_reduction` argument in `DistributedDataParallel` &quot;</span></span><br><span class="line">                <span class="string">&quot;module is deprecated. Please avoid using it.&quot;</span></span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># used for intra-node param sync and inter-node sync as well</span></span><br><span class="line">        self.broadcast_bucket_size = <span class="built_in">int</span>(<span class="number">250</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># reduction bucket size</span></span><br><span class="line">        self.bucket_bytes_cap = <span class="built_in">int</span>(bucket_cap_mb * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保证初始状态一样</span></span><br><span class="line">        <span class="comment"># Sync params and buffers</span></span><br><span class="line">        self._sync_params_and_buffers(authoritative_rank=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下拉看源码</span></span><br><span class="line">        self._ddp_init_helper()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sync_params_and_buffers</span>(<span class="params">self, authoritative_rank=<span class="number">0</span></span>):</span></span><br><span class="line">        module_states = <span class="built_in">list</span>(self.module.state_dict().values())</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(module_states) &gt; <span class="number">0</span>:</span><br><span class="line">            self._distributed_broadcast_coalesced(</span><br><span class="line">                module_states,</span><br><span class="line">                self.broadcast_bucket_size,</span><br><span class="line">                authoritative_rank)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_ddp_init_helper</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialization helper function that does the following:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        (1) replicating the module from device[0] to the other devices （前文提到 DDP 也支持一个进程多线程利用多卡，类似 DP ，这时候就会用到第一步）</span></span><br><span class="line"><span class="string">        (2) bucketing the parameters for reductions （把 parameter 分组，梯度通讯时，先得到梯度的会通讯）</span></span><br><span class="line"><span class="string">        (3) resetting the bucketing states</span></span><br><span class="line"><span class="string">        (4) registering the grad hooks （创建管理器）</span></span><br><span class="line"><span class="string">        (5) passing a handle of DDP to SyncBatchNorm Layer （为 SyncBN 准备）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">parameters</span>(<span class="params">m, recurse=<span class="literal">True</span></span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">model_parameters</span>(<span class="params">m</span>):</span></span><br><span class="line">                ps = m._former_parameters.values() \</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&quot;_former_parameters&quot;</span>) \</span><br><span class="line">                    <span class="keyword">else</span> m.parameters(recurse=<span class="literal">False</span>)</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> ps:</span><br><span class="line">                    <span class="keyword">yield</span> p</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> m.modules() <span class="keyword">if</span> recurse <span class="keyword">else</span> [m]:</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model_parameters(m):</span><br><span class="line">                    <span class="keyword">yield</span> p</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.device_ids <span class="keyword">and</span> <span class="built_in">len</span>(self.device_ids) &gt; <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">&quot;Single-Process Multi-GPU is not the recommended mode for &quot;</span></span><br><span class="line">                <span class="string">&quot;DDP. In this mode, each DDP instance operates on multiple &quot;</span></span><br><span class="line">                <span class="string">&quot;devices and creates multiple module replicas within one &quot;</span></span><br><span class="line">                <span class="string">&quot;process. The overhead of scatter/gather and GIL contention &quot;</span></span><br><span class="line">                <span class="string">&quot;in every forward pass can slow down training. &quot;</span></span><br><span class="line">                <span class="string">&quot;Please consider using one DDP instance per device or per &quot;</span></span><br><span class="line">                <span class="string">&quot;module replica by explicitly setting device_ids or &quot;</span></span><br><span class="line">                <span class="string">&quot;CUDA_VISIBLE_DEVICES. &quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># only create replicas for single-device CUDA modules</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> we don&#x27;t need to replicate params in here. they&#x27;re always going to</span></span><br><span class="line">            <span class="comment"># be broadcasted using larger blocks in broadcast_coalesced, so it might be</span></span><br><span class="line">            <span class="comment"># better to not pollute the caches with these small blocks</span></span><br><span class="line">            self._module_copies = replicate(self.module, self.device_ids, detach=<span class="literal">True</span>)</span><br><span class="line">            self._module_copies[<span class="number">0</span>] = self.module</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> module_copy <span class="keyword">in</span> self._module_copies[<span class="number">1</span>:]:</span><br><span class="line">                <span class="keyword">for</span> param, copy_param <span class="keyword">in</span> <span class="built_in">zip</span>(self.module.parameters(), parameters(module_copy)):</span><br><span class="line">                    <span class="comment"># Reducer requires param copies have the same strides across replicas.</span></span><br><span class="line">                    <span class="comment"># Fixes up copy_param strides in case replicate didn&#x27;t match param strides.</span></span><br><span class="line">                    <span class="keyword">if</span> param.layout <span class="keyword">is</span> torch.strided <span class="keyword">and</span> param.stride() != copy_param.stride():</span><br><span class="line">                        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                            copy_param.set_(copy_param.clone()</span><br><span class="line">                                                      .as_strided(param.size(), param.stride())</span><br><span class="line">                                                      .copy_(copy_param))</span><br><span class="line">                    copy_param.requires_grad = param.requires_grad</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._module_copies = [self.module]</span><br><span class="line"></span><br><span class="line">        self.modules_params = [<span class="built_in">list</span>(parameters(m)) <span class="keyword">for</span> m <span class="keyword">in</span> self._module_copies]</span><br><span class="line">        self.modules_buffers = [<span class="built_in">list</span>(m.buffers()) <span class="keyword">for</span> m <span class="keyword">in</span> self._module_copies]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build tuple of (module, parameter) for all parameters that require grads.</span></span><br><span class="line">        modules_and_parameters = [</span><br><span class="line">            [</span><br><span class="line">                (module, parameter)</span><br><span class="line">                <span class="keyword">for</span> module <span class="keyword">in</span> replica.modules()</span><br><span class="line">                <span class="keyword">for</span> parameter <span class="keyword">in</span> <span class="built_in">filter</span>(</span><br><span class="line">                    <span class="keyword">lambda</span> parameter: parameter.requires_grad,</span><br><span class="line">                    parameters(module, recurse=<span class="literal">False</span>))</span><br><span class="line">            ] <span class="keyword">for</span> replica <span class="keyword">in</span> self._module_copies]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build list of parameters.</span></span><br><span class="line">        parameters = [</span><br><span class="line">            <span class="built_in">list</span>(parameter <span class="keyword">for</span> _, parameter <span class="keyword">in</span> replica)</span><br><span class="line">            <span class="keyword">for</span> replica <span class="keyword">in</span> modules_and_parameters]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Checks if a module will produce a sparse gradient.</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">produces_sparse_gradient</span>(<span class="params">module</span>):</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, torch.nn.Embedding):</span><br><span class="line">                <span class="keyword">return</span> module.sparse</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, torch.nn.EmbeddingBag):</span><br><span class="line">                <span class="keyword">return</span> module.sparse</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build list of booleans indicating whether or not to expect sparse</span></span><br><span class="line">        <span class="comment"># gradients for the corresponding parameters.</span></span><br><span class="line">        expect_sparse_gradient = [</span><br><span class="line">            <span class="built_in">list</span>(produces_sparse_gradient(module) <span class="keyword">for</span> module, _ <span class="keyword">in</span> replica)</span><br><span class="line">            <span class="keyword">for</span> replica <span class="keyword">in</span> modules_and_parameters]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The bucket size limit is specified in the constructor.</span></span><br><span class="line">        <span class="comment"># Additionally, we allow for a single small bucket for parameters</span></span><br><span class="line">        <span class="comment"># that are defined first, such that their gradients don&#x27;t spill into</span></span><br><span class="line">        <span class="comment"># a much larger bucket, adding unnecessary latency after gradient</span></span><br><span class="line">        <span class="comment"># computation finishes. Experiments showed 1MB is a reasonable value.</span></span><br><span class="line">        bucket_indices = dist._compute_bucket_assignment_by_size(</span><br><span class="line">            parameters[<span class="number">0</span>],</span><br><span class="line">            [dist._DEFAULT_FIRST_BUCKET_BYTES, self.bucket_bytes_cap],</span><br><span class="line">            expect_sparse_gradient[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: reverse list of buckets because we want to approximate the</span></span><br><span class="line">        <span class="comment"># order in which their gradients are produced, and assume they</span></span><br><span class="line">        <span class="comment"># are used in the forward pass in the order they are defined.</span></span><br><span class="line">        <span class="comment"># 管理器</span></span><br><span class="line">        self.reducer = dist.Reducer(</span><br><span class="line">            parameters,</span><br><span class="line">            <span class="built_in">list</span>(<span class="built_in">reversed</span>(bucket_indices)),</span><br><span class="line">            self.process_group,</span><br><span class="line">            expect_sparse_gradient,</span><br><span class="line">            self.bucket_bytes_cap,</span><br><span class="line">            self.find_unused_parameters,</span><br><span class="line">            self.gradient_as_bucket_view)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># passing a handle to torch.nn.SyncBatchNorm layer</span></span><br><span class="line">        self._passing_sync_batchnorm_handle(self._module_copies)</span><br></pre></td></tr></table></figure>
<p>每个 DDP 进程都会创建本地 Reducer 在 backward 时管理梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">self.reducer = dist.Reducer(</span><br><span class="line">     parameters,</span><br><span class="line">     <span class="built_in">list</span>(<span class="built_in">reversed</span>(bucket_indices)),</span><br><span class="line">     self.process_group,</span><br><span class="line">     expect_sparse_gradient,</span><br><span class="line">     self.bucket_bytes_cap,</span><br><span class="line">     self.find_unused_parameters,</span><br><span class="line">     self.gradient_as_bucket_view)</span><br></pre></td></tr></table></figure>
<p>我们看 Reducer.cpp 可以发现，构建 Reducer 时，除了各种初始化，最重要的一步就是</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">auto</span> replica_count = replicas_.<span class="built_in">size</span>();</span><br><span class="line">  grad_accumulators_.<span class="built_in">resize</span>(replica_count);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> replica_index = <span class="number">0</span>; replica_index &lt; replica_count;</span><br><span class="line">       replica_index++) &#123;         </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span> variable_count = replicas_[replica_index].<span class="built_in">size</span>();</span><br><span class="line">    grad_accumulators_[replica_index].<span class="built_in">resize</span>(variable_count);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> variable_index = <span class="number">0</span>; variable_index &lt; variable_count;</span><br><span class="line">         variable_index++) </span><br><span class="line">    &#123; </span><br><span class="line">      <span class="keyword">auto</span>&amp; variable = replicas_[replica_index][variable_index];  </span><br><span class="line">      <span class="keyword">const</span> <span class="keyword">auto</span> index = <span class="built_in">VariableIndex</span>(replica_index, variable_index);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// The gradient accumulator function is lazily initialized once.</span></span><br><span class="line">      <span class="comment">// Therefore we can use its presence in the autograd graph as</span></span><br><span class="line">      <span class="comment">// evidence that the parameter has participated in an iteration.</span></span><br><span class="line">      <span class="keyword">auto</span> grad_accumulator =</span><br><span class="line">          torch::autograd::impl::<span class="built_in">grad_accumulator</span>(variable);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _WIN32</span></span><br><span class="line">        <span class="keyword">using</span> torch::distributed::autograd::ThreadLocalDistAutogradContext;   </span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">        <span class="comment">// grad_accumulator 执行完后，autograd_hook 就会运行</span></span><br><span class="line">        hooks.<span class="built_in">emplace_back</span>(</span><br><span class="line">            grad_accumulator-&gt;<span class="built_in">add_post_hook</span>(</span><br><span class="line">                torch::make_unique&lt;torch::autograd::utils::LambdaPostHook&gt;(</span><br><span class="line">                    [=](<span class="keyword">const</span> torch::autograd::variable_list&amp; outputs,</span><br><span class="line">                        <span class="keyword">const</span> torch::autograd::variable_list&amp; <span class="comment">/* unused */</span>)&#123;   </span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> WIN32</span></span><br><span class="line">                         <span class="keyword">this</span>-&gt;rpc_context.<span class="built_in">set</span>(</span><br><span class="line">                             ThreadLocalDistAutogradContext::<span class="built_in">getContextPtr</span>());   </span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">                         <span class="keyword">this</span>-&gt;<span class="built_in">autograd_hook</span>(index);</span><br><span class="line">                         <span class="keyword">return</span> outputs;</span><br><span class="line">                       &#125;)),</span><br><span class="line">               grad_accumulator);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Map raw function pointer to replica index and parameter index.</span></span><br><span class="line">          <span class="comment">// This is used later on when the autograd graph is traversed</span></span><br><span class="line">          <span class="comment">// to check for parameters for which no gradient is computed.</span></span><br><span class="line">          func_[grad_accumulator.<span class="built_in">get</span>()] = index;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// The gradient accumulator is stored as weak_ptr in the autograd</span></span><br><span class="line">          <span class="comment">// metadata of the variable, so we have to keep it alive here for</span></span><br><span class="line">          <span class="comment">// the raw pointer to be valid.</span></span><br><span class="line">          grad_accumulators_[replica_index][variable_index] =</span><br><span class="line">              std::<span class="built_in">move</span>(grad_accumulator);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// std::unordered_map&lt;torch::autograd::Node*, VariableIndex&gt; func_;</span></span><br><span class="line">    <span class="comment">// func_ 存了grad_accumulator &amp; index 的对应，方便我们之后在 autograd graph 寻找 unused parameters</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//  std::vector&lt;std::vector&lt;std::shared_ptr&lt;torch::autograd::Node&gt;&gt;&gt;</span></span><br><span class="line">    <span class="comment">//  grad_accumulators_;</span></span><br><span class="line">    <span class="comment">//  grad_accumulators_ 对应的 index 存了相应的 grad_accumulator</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//   std::vector&lt;std::pair&lt;uintptr_t, std::shared_ptr&lt;torch::autograd::Node&gt;&gt;&gt;</span></span><br><span class="line">    <span class="comment">//   hooks_;</span></span><br></pre></td></tr></table></figure>
<p>其中，发挥重要功能的 autograd_hook 如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Reducer::autograd_hook</span><span class="params">(VariableIndex index)</span> </span>&#123;</span><br><span class="line">     <span class="function">std::lock_guard <span class="title">lock</span><span class="params">(<span class="keyword">this</span>-&gt;mutex_)</span></span>;</span><br><span class="line">     <span class="keyword">if</span> (find_unused_parameters_) &#123;</span><br><span class="line">       <span class="comment">// 在 no_sync 时，只要参数被用过一次，就会被标记为用过</span></span><br><span class="line">       <span class="comment">// Since it gets here, this param has been used for this iteration. We want</span></span><br><span class="line">       <span class="comment">// to mark it in local_used_maps_. During no_sync session, the same var can</span></span><br><span class="line">       <span class="comment">// be set multiple times, which is OK as does not affect correctness. As</span></span><br><span class="line">       <span class="comment">// long as it is used once during no_sync session, it is marked as used.</span></span><br><span class="line">       local_used_maps_[index.replica_index][index.variable_index] = <span class="number">1</span>;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ignore if we don&#x27;t expect to be called.</span></span><br><span class="line">    <span class="comment">// This may be the case if the user wants to accumulate gradients</span></span><br><span class="line">    <span class="comment">// for number of iterations before reducing them.</span></span><br><span class="line">    <span class="keyword">if</span> (!expect_autograd_hooks_) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Rebuild bucket only if 1) it is the first time to rebuild bucket 2)</span></span><br><span class="line">    <span class="comment">// find_unused_parameters_ is false, currently it does not support when there</span></span><br><span class="line">    <span class="comment">// are unused parameters 3) this backward pass needs to run allreduce. Here,</span></span><br><span class="line">    <span class="comment">// we just dump tensors and their parameter indices into rebuilt_params_ and</span></span><br><span class="line">    <span class="comment">// rebuilt_param_indices_ based on gradient arriving order, and then at the</span></span><br><span class="line">    <span class="comment">// end of finalize_backward(), buckets will be rebuilt based on</span></span><br><span class="line">    <span class="comment">// rebuilt_params_ and rebuilt_param_indices_, and then will be broadcasted</span></span><br><span class="line">    <span class="comment">// and initialized. Also we only need to dump tensors and parameter indices of</span></span><br><span class="line">    <span class="comment">// one replica.</span></span><br><span class="line">    <span class="built_in">push_rebuilt_params</span>(index);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If `find_unused_parameters_` is true there may be model parameters that</span></span><br><span class="line">    <span class="comment">// went unused when computing the model output, they won&#x27;t be part of the</span></span><br><span class="line">    <span class="comment">// autograd graph, and won&#x27;t receive gradients. These parameters are</span></span><br><span class="line">    <span class="comment">// discovered in the `prepare_for_backward` function and their indexes stored</span></span><br><span class="line">    <span class="comment">// in the `unused_parameters_` vector.</span></span><br><span class="line">    <span class="keyword">if</span> (!has_marked_unused_parameters_ &amp;&amp; find_unused_parameters_) &#123;</span><br><span class="line">      has_marked_unused_parameters_ = <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; unused_index : unused_parameters_) &#123;</span><br><span class="line">        <span class="built_in">mark_variable_ready</span>(unused_index);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Finally mark variable for which this function was originally called.</span></span><br><span class="line">    <span class="built_in">mark_variable_ready</span>(index);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>前向传播</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, *kwargs</span>):</span>           <span class="keyword">if</span> self.ddp_join_enabled:               ones = torch.ones(                   <span class="number">1</span>, device=self.device               )               work = dist.all_reduce(ones, group=self.process_group, async_op=<span class="literal">True</span>)               self.reducer._set_forward_pass_work_handle(                   work, self.ddp_join_divide_by_initial_world_size               )</span><br><span class="line"><span class="comment"># Calling _rebuild_buckets before forward compuation,</span></span><br><span class="line">      <span class="comment"># It may allocate new buckets before deallocating old buckets</span></span><br><span class="line">      <span class="comment"># inside _rebuild_buckets. To save peak memory usage,</span></span><br><span class="line">      <span class="comment"># call _rebuild_buckets before the peak memory usage increases</span></span><br><span class="line">      <span class="comment"># during forward computation.</span></span><br><span class="line">      <span class="comment"># This should be called only once during whole training period.</span></span><br><span class="line">      <span class="keyword">if</span> self.reducer._rebuild_buckets():</span><br><span class="line">          logging.info(<span class="string">&quot;Reducer buckets have been rebuilt in this iteration.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.require_forward_param_sync:</span><br><span class="line">          self._sync_params()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.ddp_join_enabled:</span><br><span class="line">          <span class="comment"># Notify joined ranks whether they should sync in backwards pass or not.</span></span><br><span class="line">          self._check_global_requires_backward_grad_sync(is_joined_rank=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># ！！！</span></span><br><span class="line">      <span class="keyword">if</span> self.device_ids:</span><br><span class="line">          inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line">          <span class="keyword">if</span> <span class="built_in">len</span>(self.device_ids) == <span class="number">1</span>:</span><br><span class="line">              output = self.module(*inputs[<span class="number">0</span>], **kwargs[<span class="number">0</span>])</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 单进程多线程多卡的情况</span></span><br><span class="line">              outputs = self.parallel_apply(self._module_copies[:<span class="built_in">len</span>(inputs)], inputs, kwargs)</span><br><span class="line">              output = self.gather(outputs, self.output_device)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          output = self.module(*inputs, **kwargs)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> torch.is_grad_enabled() <span class="keyword">and</span> self.require_backward_grad_sync:</span><br><span class="line">          self.require_forward_param_sync = <span class="literal">True</span></span><br><span class="line">          <span class="comment"># We&#x27;ll return the output object verbatim since it is a freeform</span></span><br><span class="line">          <span class="comment"># object. We need to find any tensors in this object, though,</span></span><br><span class="line">          <span class="comment"># because we need to figure out which parameters were used during</span></span><br><span class="line">          <span class="comment"># this forward pass, to ensure we short circuit reduction for any</span></span><br><span class="line">          <span class="comment"># unused parameters. Only if `find_unused_parameters` is set.</span></span><br><span class="line">          <span class="keyword">if</span> self.find_unused_parameters:</span><br><span class="line">          <span class="comment"># 当DDP参数 find_unused_parameter 为 true 时，其会在 forward 结束时，启动一个回溯，标记出所有没被用到的 parameter，提前把这些设定为 ready，这样 backward 就可以在一个 subgraph 进行，但这样会牺牲一部分时间。</span></span><br><span class="line">              self.reducer.prepare_for_backward(<span class="built_in">list</span>(_find_tensors(output)))</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              self.reducer.prepare_for_backward([])</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self.require_forward_param_sync = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<ul>
<li>反向传播</li>
</ul>
<p>那么，DDP 究竟是怎么启动 allreduce 的呢？我们看一下 reducer.cpp 里对桶的定义以及用法，主要是在mark_*_ready。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Bucket</span> &#123;</span>       std::vector replicas;</span><br><span class="line"><span class="comment">// Global indices of participating variables in the bucket</span></span><br><span class="line">  std::vector&lt;<span class="keyword">size_t</span>&gt; variable_indices;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Number of replicas to be marked done before this bucket is ready.</span></span><br><span class="line">  <span class="comment">// 计数</span></span><br><span class="line">  <span class="keyword">size_t</span> pending;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Keep work handle around when this set of buckets is being reduced.</span></span><br><span class="line">  std::shared_ptr&lt;c10d::ProcessGroup::Work&gt; work;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Keep future work handle around if DDP comm hook is registered.</span></span><br><span class="line">  c10::intrusive_ptr&lt;torch::jit::Future&gt; future_work;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this bucket should expect a single sparse gradient.</span></span><br><span class="line">  <span class="comment">// Implies: replicas[i].variables.size() == 1.</span></span><br><span class="line">  <span class="keyword">bool</span> expect_sparse_gradient = <span class="literal">false</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>先看 mark_variable_ready，截取片段（指去除报错信息）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Reducer::mark_variable_ready</span><span class="params">(VariableIndex index)</span> </span>&#123;     <span class="keyword">const</span> <span class="keyword">auto</span> replica_index = index.replica_index;     <span class="keyword">const</span> <span class="keyword">auto</span> variable_index = index.variable_index;     <span class="built_in">TORCH_CHECK</span>(replica_index &lt; replicas_.<span class="built_in">size</span>(), <span class="string">&quot;Out of range replica index.&quot;</span>);     <span class="built_in">TORCH_CHECK</span>(         variable_index &lt; variable_locators_.<span class="built_in">size</span>(),         <span class="string">&quot;Out of range variable index.&quot;</span>);     backward_stats_[replica_index][variable_index] =         <span class="built_in">current_time_in_nanos</span>() - backward_stats_base_;</span><br><span class="line"><span class="comment">// 每当变量被标记成 ready 了，都要调用一下 finalize</span></span><br><span class="line">require_finalize_ = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">auto</span>&amp; bucket_index = variable_locators_[variable_index];</span><br><span class="line"><span class="keyword">auto</span>&amp; bucket = buckets_[bucket_index.bucket_index];</span><br><span class="line"><span class="keyword">auto</span>&amp; replica = bucket.replicas[replica_index];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// If it was scheduled, wait on allreduce in forward pass that tells us</span></span><br><span class="line"><span class="comment">// division factor based on no. of currently participating processes.</span></span><br><span class="line"><span class="keyword">if</span> (divFactor_ == kUnsetDivFactor) &#123;</span><br><span class="line">  divFactor_ = process_group_-&gt;<span class="built_in">getSize</span>();</span><br><span class="line">  <span class="keyword">auto</span>&amp; workHandle = forwardPassWorkHandle_.workHandle;</span><br><span class="line">  <span class="keyword">if</span> (workHandle &amp;&amp; !forwardPassWorkHandle_.useStaticWorldSize) &#123;</span><br><span class="line">    workHandle-&gt;<span class="built_in">wait</span>();</span><br><span class="line">    <span class="keyword">auto</span> results = workHandle-&gt;<span class="built_in">result</span>();</span><br><span class="line">    <span class="comment">// Guard against the results being empty</span></span><br><span class="line">    <span class="built_in">TORCH_INTERNAL_ASSERT</span>(results.<span class="built_in">size</span>() &gt; <span class="number">0</span>);</span><br><span class="line">    at::Tensor&amp; res = results.<span class="built_in">front</span>();</span><br><span class="line">    divFactor_ = res.<span class="built_in">item</span>().to&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (bucket.expect_sparse_gradient) &#123;</span><br><span class="line">  <span class="built_in">mark_variable_ready_sparse</span>(index);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">mark_variable_ready_dense</span>(index);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 检查桶里的变量是不是都ready了，如果没有东西 pending，那就是都 ready了</span></span><br><span class="line"><span class="keyword">if</span> (--replica.pending == <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (--bucket.pending == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">mark_bucket_ready</span>(bucket_index.bucket_index);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Run finalizer function and kick off reduction for local_used_maps once the</span></span><br><span class="line"><span class="comment">// final bucket was marked ready.</span></span><br><span class="line"><span class="keyword">if</span> (next_bucket_ == buckets_.<span class="built_in">size</span>()) &#123;</span><br><span class="line">  <span class="keyword">if</span> (find_unused_parameters_) &#123;</span><br><span class="line">    <span class="comment">// H2D from local_used_maps_ to local_used_maps_dev_</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; local_used_maps_.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">      <span class="comment">// We do async H2D to avoid the blocking overhead. The async copy and</span></span><br><span class="line">      <span class="comment">// allreduce respect the current stream, so will be sequenced correctly.</span></span><br><span class="line">      local_used_maps_dev_[i].<span class="built_in">copy_</span>(local_used_maps_[i], <span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    local_used_work_ = process_group_-&gt;<span class="built_in">allreduce</span>(local_used_maps_dev_);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The autograd engine uses the default stream when running callbacks, so we</span></span><br><span class="line">  <span class="comment">// pass in the current CUDA stream in case it is not the default.</span></span><br><span class="line">  c10::DeviceType deviceType = replica.contents.<span class="built_in">device</span>().<span class="built_in">type</span>();</span><br><span class="line">  <span class="keyword">const</span> c10::impl::VirtualGuardImpl guard =</span><br><span class="line">      c10::impl::VirtualGuardImpl&#123;deviceType&#125;;</span><br><span class="line">  <span class="keyword">const</span> c10::Stream currentStream =</span><br><span class="line">      guard.<span class="built_in">getStream</span>(replica.contents.<span class="built_in">device</span>());</span><br><span class="line">  torch::autograd::Engine::<span class="built_in">get_default_engine</span>().<span class="built_in">queue_callback</span>([=] &#123;</span><br><span class="line">    std::lock_guard&lt;std::mutex&gt; <span class="built_in">lock</span>(<span class="keyword">this</span>-&gt;mutex_);</span><br><span class="line">    <span class="comment">// Run callback with the current stream</span></span><br><span class="line">    c10::OptionalStreamGuard currentStreamGuard&#123;currentStream&#125;;</span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">finalize_backward</span>();</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Reducer::mark_bucket_ready</span><span class="params">(<span class="keyword">size_t</span> bucket_index)</span> </span>&#123;     <span class="built_in">TORCH_INTERNAL_ASSERT</span>(bucket_index &gt;= next_bucket_);</span><br><span class="line"><span class="comment">// Buckets are reduced in sequence. Ignore this bucket if</span></span><br><span class="line"><span class="comment">// it&#x27;s not its turn to be reduced.</span></span><br><span class="line"><span class="keyword">if</span> (bucket_index &gt; next_bucket_) &#123;</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Keep going, until we either:</span></span><br><span class="line"><span class="comment">// - 所有桶都在 allreduce 那就等着 or</span></span><br><span class="line"><span class="comment">// - 还有桶没好，那也等着.</span></span><br><span class="line"><span class="keyword">for</span> (; next_bucket_ &lt; buckets_.<span class="built_in">size</span>() &amp;&amp; buckets_[next_bucket_].pending == <span class="number">0</span>;</span><br><span class="line">     next_bucket_++) &#123;</span><br><span class="line">  <span class="keyword">auto</span>&amp; bucket = buckets_[next_bucket_];</span><br><span class="line">  std::vector&lt;at::Tensor&gt; tensors;</span><br><span class="line">  tensors.<span class="built_in">reserve</span>(bucket.replicas.<span class="built_in">size</span>());</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; replica : bucket.replicas) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// CUDA default stream 都按时序排好了</span></span><br><span class="line">    tensors.<span class="built_in">push_back</span>(replica.contents);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (comm_hook_ == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    <span class="comment">// 如果没注册 comm_hook，直接 allreduce</span></span><br><span class="line">    bucket.work = process_group_-&gt;<span class="built_in">allreduce</span>(tensors);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 注册了 comm_hook 那就先跑 hook</span></span><br><span class="line">    <span class="comment">// 需要注意的是，这个comm_hook 只是处理通信的底层hook，如果想在 reduce 前分别进行梯度裁剪，还是需要在 autograph 挂 hook</span></span><br><span class="line">    bucket.future_work = comm_hook_-&gt;<span class="built_in">runHook</span>(<span class="built_in">GradBucket</span>(tensors));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>除了正常的前向传播，DDP 还允许在 subgraph 进行反向传播，只需将 self.find_unused_parameters 设置为 True。或许有朋友会问，如果 find_unused_parameters 设置为 True，那每次都要 traverse 计算图，明明开销很大，为什么有时候我们还要将 self.find_unused_parameters 设置为 True？ 这是因为训练时有可能某次迭代只用到整个模型的一个 subgraph， 并且这个 subgraph 迭代时可能会改变，就是说某些参数可能会在训练时被跳过。但因为所有parameters 在一开始就被分好桶了，而我们的 hook 又规定了只有整个桶 ready 了（pending==0）才会通信，如果我们不将 unused parameter 标记为 ready，整个过程会没法进行。我们在这节结束的部分附上一个小实验验证一下。</p>
<p>DDP 通过在构建时注册 autograd hook 进行梯度同步。当一个梯度计算好后，相应的 hook 会告诉 DDP 可以用来归约。当一个桶里的梯度都可以了，Reducer 就会启动异步 allreduce 去计算所有进程的平均值。当所有桶都可以了，Reducer 会等所有 allreduce 完成，然后将得到的梯度写到 param.grad。</p>
<p>optimizer step 独立于 DDP，所有进程的模型能够同步是因为初始状态相同并且改变量也相同。</p>
<ul>
<li>实验：find_unused_params</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os   </span><br><span class="line"><span class="keyword">import</span> torch   </span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist   </span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp   </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn   </span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim   </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"><span class="keyword">from</span> timeit <span class="keyword">import</span> default_timer <span class="keyword">as</span> timer</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="string">&#x27;localhost&#x27;</span>   </span><br><span class="line">os.environ[<span class="string">&#x27;MASTER_PORT&#x27;</span>] = <span class="string">&#x27;12138&#x27;</span>   </span><br><span class="line"><span class="comment"># sync   </span></span><br><span class="line">seed = <span class="number">0</span>   </span><br><span class="line">torch.manual_seed(seed)   </span><br><span class="line">torch.cuda.manual_seed(seed)   </span><br><span class="line">torch.cuda.manual_seed_all(seed)   </span><br><span class="line">os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)   </span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span>   </span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example</span>(<span class="params">rank, world_size</span>):</span>       </span><br><span class="line">    <span class="comment"># create default process group       </span></span><br><span class="line">    dist.init_process_group(<span class="string">&quot;gloo&quot;</span>,rank=rank, </span><br><span class="line">world_size=world_size,init_method=<span class="string">&#x27;env://&#x27;</span>)       </span><br><span class="line">    <span class="comment"># create local model</span></span><br><span class="line">    model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(rank)</span><br><span class="line">    <span class="comment"># construct DDP model</span></span><br><span class="line">    ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line">    <span class="comment"># define loss function and optimizer</span></span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    buf = <span class="number">0</span></span><br><span class="line">    tmp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">        start = timer()</span><br><span class="line">        <span class="comment"># forward pass</span></span><br><span class="line">        outputs = ddp_model(torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(rank))</span><br><span class="line">        end = timer()</span><br><span class="line"></span><br><span class="line">        tmp = end-start</span><br><span class="line">        buf+=tmp</span><br><span class="line">        labels = torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(rank)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        loss_fn(outputs, labels).backward()</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(tmp)</span><br><span class="line">    <span class="built_in">print</span>(buf)</span><br><span class="line">    <span class="built_in">print</span>(buf/<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    world_size = <span class="number">1</span></span><br><span class="line">    mp.spawn(example,</span><br><span class="line">        args=(world_size,),</span><br><span class="line">        nprocs=world_size,</span><br><span class="line">        join=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">     main()</span><br></pre></td></tr></table></figure>
<p>将 find_unused_params 分别设置成 True 或者 False 跑多次取平均，可以得到：</p>
<ul>
<li>find_unused_params=True: 0.3367 ms</li>
<li>find_unused_params=False: 0.2993 ms</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DP/" rel="tag"># DP</a>
              <a href="/tags/DDP/" rel="tag"># DDP</a>
              <a href="/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/" rel="tag"># 源码解读</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" rel="tag"># 分布式训练</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/PyTorch%E4%B8%AD%E5%8F%8D%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%90%86%E8%A7%A3/" rel="prev" title="PyTorch中反卷积的理解">
      <i class="fa fa-chevron-left"></i> PyTorch中反卷积的理解
    </a></div>
      <div class="post-nav-item">
    <a href="/LeetCode%E7%AC%AC%E4%B8%80%E9%A2%98%EF%BC%9A%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/" rel="next" title="LeetCode第一题：两数之和">
      LeetCode第一题：两数之和 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="nav-number">1.</span> <span class="nav-text">数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DP"><span class="nav-number">2.</span> <span class="nav-text">DP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8"><span class="nav-number">2.1.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.3.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">2.4.</span> <span class="nav-text">分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDP"><span class="nav-number">3.</span> <span class="nav-text">DDP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-2"><span class="nav-number">3.1.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86-2"><span class="nav-number">3.2.</span> <span class="nav-text">原理</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zsh"
      src="/images/header2.jpg">
  <p class="site-author-name" itemprop="name">zsh</p>
  <div class="site-description" itemprop="description">怕什么真理无穷<br>进一寸有一寸的欢喜</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">85</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zsh4614" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zsh4614" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.jianshu.com/u/f513362c9d79" title="简书 → https:&#x2F;&#x2F;www.jianshu.com&#x2F;u&#x2F;f513362c9d79" rel="noopener" target="_blank"><i class="fas fa-book fa-fw"></i>简书</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zsh4614@gmail.com" title="E-Mail → mailto:zsh4614@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/cjhfhb" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;cjhfhb" rel="noopener" target="_blank"><i class="fa fa-globe-americas fa-fw"></i>知乎</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="powered-by">
    水滴石穿，绳锯木断。
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      已有<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人访问
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      总访问<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次
    </span>
  
</div>









      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clickloves.js"></script>
